{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Đọc/ghi file Parquet trong Hadoop sử dụng mô hình MapReduce\n",
    "\n",
    "* Sử dụng thư viện Apache Spark trên EMR để tiến hành đọc file .csv từ Amazon S3 và chuyển thành file .parquet và cũng dùng chính thư viện này để tiến hành đọc/ghi file .parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Kiểm tra kết nối giữa Cluster của EMR và Instance của SageMaker Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Tiến hành đọc file .csv từ S3 và hiển thị kết quả, đồng thời cho biến thời gian để lấy thời gian đọc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "//Biến tính thời gian\n",
    "val t1 = System.nanoTime\n",
    "\n",
    "//Tạo biến df chứa dataframe của file .csv đọc được từ S3\n",
    "val df = spark.read.csv(\"s3://hadoopbucket177103/input/train.csv\")\n",
    "\n",
    "//In thời gian thực thi câu lệnh trên\n",
    "val duration1 = (System.nanoTime - t1) / 1e9d\n",
    "\n",
    "//In kết quả có được sau khi đọc file .csv (20 dòng đầu tiên)\n",
    "    df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Ghi Dataframe vào file .csv lưu trữ trên Amazon S3, đồng thời xác định thời gian để thực thi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "val t2 = System.nanoTime\n",
    "\n",
    "//Ghi Dataframe vào file .csv sử dụng overwrite save mode\n",
    "df.write.mode(\"overwrite\").csv(\"s3://hadoopbucket177103/output/train.csv\")\n",
    "\n",
    "val duration2 = (System.nanoTime - t2) / 1e9d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Ghi Dataframe vào file .parquet lưu trữ trên Amazon S3, đồng thời xác định thời gian để thực thi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "// //Ghi Dataframe vào file .parquet\n",
    "//df.write.parquet(\"s3://hadoopbucket177103/output/train.parquet\")\n",
    "\n",
    "val t3 = System.nanoTime\n",
    "\n",
    "//Ghi Dataframe vào file .parquet sử dụng overwrite save mode\n",
    "df.write.mode(\"overwrite\").parquet(\"s3://hadoopbucket177103/output/train.parquet\")\n",
    "\n",
    "val duration3 = (System.nanoTime - t3) / 1e9d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Đọc file .parquet trên Amazon S3 và đưa vào Dataframe, đồng thời xác định thời gian để thực thi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "scala"
    }
   },
   "outputs": [],
   "source": [
    "val t4 = System.nanoTime\n",
    "\n",
    "//Tạo biến df2 chứa dataframe của file .parquet đọc được từ S3\n",
    "val df2 = spark.read.parquet(\"s3://hadoopbucket177103/output/train.parquet\")\n",
    "\n",
    "val duration4 = (System.nanoTime - t4) / 1e9d\n",
    "\n",
    "//Hiển thị kết quả lấy được từ file .parquet trong Dataframe\n",
    "df2.show();"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
